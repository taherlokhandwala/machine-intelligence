{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Functions to preprocess the dataset\n",
    "'''\n",
    "\n",
    "# normalises the columns specified for that dataframe\n",
    "def normalize(df, columns):\n",
    "    result = df.copy()\n",
    "    for feature_name in columns:\n",
    "        max_value = df[feature_name].max()\n",
    "        min_value = df[feature_name].min()\n",
    "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result\n",
    "\n",
    "# clean the passed data frame\n",
    "def cleaning(df):\n",
    "    df=df.dropna(thresh=7)\n",
    "    df[\"Age\"] = df.groupby(\"Community\").transform(lambda x: x.fillna(round(x.mean())))\n",
    "    df['Weight'] = df['Weight'].fillna(df.groupby('Age')['Weight'].transform('mean'))\n",
    "    df['Delivery phase'] = df['Delivery phase'].fillna(round((df.groupby('Age')['Delivery phase'].transform('mean'))))\n",
    "    df['Delivery phase'] -= 1\n",
    "    df['HB'] = df['HB'].fillna(round((df.groupby('Age')['HB'].transform('mean'))))\n",
    "    df['BP'] = df['BP'].fillna(df.groupby('Weight')['BP'].transform('mean'))\n",
    "    df=df.fillna(method=\"ffill\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef back_propagation(x, y): \\n    global W1, W2, alpha, momentum, history1, history2\\n    # hiden layer \\n    z1 = x.dot(W1)# input from layer 1 \\n    a1 = sigmoid(z1)# output of layer 2 \\n    \\n    # Output layer \\n    z2 = a1.dot(W2)# input of out layer \\n    a2 = sigmoid(z2)# output of out layer \\n    # error in output layer \\n    d2 =(a2-y) \\n    d1 = np.multiply((W2.dot((d2.transpose()))).transpose(), \\n                        (np.multiply(a1, 1-a1))) \\n\\n    # Gradient for w1 and w2 \\n    w1_adj = x.transpose().dot(d1) \\n    w2_adj = a1.transpose().dot(d2) \\n    \\n    # Updating parameters\\n    history1 = momentum*history1 + alpha*(w1_adj)\\n    W1 = W1 - history1\\n    history2 = momentum*history2 + alpha*(w2_adj)\\n    W2 = W2 - history2\\n\\n    return(W1, W2) \\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Functions used to train the Neural Network\n",
    "'''\n",
    "\n",
    "# initializing the weights randomly \n",
    "def generate_wt(x, y): \n",
    "    r_weights =[] \n",
    "    for i in range(x * y): \n",
    "        r_weights.append(np.random.randn()) \n",
    "    return(np.array(r_weights).reshape(x, y))\n",
    "\n",
    "# sigmoid activation function\n",
    "def sigmoid(x, derivative = False):\n",
    "    if derivative:\n",
    "        return np.multiply(sigmoid(x),(1-sigmoid(x)))\n",
    "    return(1/(1 + np.exp(-x))) \n",
    "\n",
    "# ReLu activation function\n",
    "def ReLU(x, derivative = False):\n",
    "    if derivative:\n",
    "        return 1 if x.flatten()[0] > 0 else 0\n",
    "    return x * (x.flatten()[0] > 0)\n",
    "\n",
    "# Swish activation function\n",
    "def swish(x, derivative = False):\n",
    "    if derivative:\n",
    "        return sigmoid(x) + x*sigmoid(x, derivative=True)\n",
    "    return x*sigmoid(x)\n",
    "\n",
    "# tanh activation function\n",
    "def tanh_f(x, derivative = False):\n",
    "    if derivative:\n",
    "        return 1 - np.power(x,2)\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def loss(y_hats, Y): \n",
    "    '''\n",
    "    s =(np.square(out-Y))\n",
    "    s = np.sum(s)/2\n",
    "    return(s)\n",
    "    '''\n",
    "    #return 0.5*np.sum((out-Y)**2)\n",
    "'''\n",
    "# get loss for epoch\n",
    "def get_loss(y,y_hat):\n",
    "    loss_vals = []\n",
    "    y = list(y)\n",
    "    for i in range(len(y_hat)):\n",
    "        loss_vals.append((loss(y_hat[i],y[i],2))\n",
    "    print(loss_vals)\n",
    "    loss_val = (sum(loss_vals)/len(y))*100\n",
    "    return loss_val\n",
    "'''\n",
    "def compute_loss(y_hats, Y):\n",
    "    #print(type(y_hats))\n",
    "    #global parameters\n",
    "    return 0.5*np.sum((y_hats-Y)**2)\n",
    "\n",
    "# accuracy for binary classification\n",
    "def compute_acc(y_hats, Y):\n",
    "    tp,tn,fp,fn = 0,0,0,0\n",
    "    #print(len(y_hats), len(Y))\n",
    "    for i in range(len(Y)):\n",
    "        y_hats[i] = 1 if y_hats[i] > 0.6 else 0\n",
    "        if(Y[i]==1 and y_hats[i]==1):\n",
    "            tp=tp+1\n",
    "        if(Y[i]==0 and y_hats[i]==0):\n",
    "            tn=tn+1\n",
    "    return (tp+tn)/len(Y)\n",
    "def feed_forward_prop_backward(x_i, do_dropout, y_i, train=False):\n",
    "    global W1, W2, dropout_percent,hidden_dim,alpha, momentum, history1, history2\n",
    "    # hidden \n",
    "    # multiply each x_i with each neuron of hidden layer\n",
    "    activation1 = x_i.dot(W1)\n",
    "    \n",
    "    # pass product of weights and x_i to activation function \n",
    "    output1 = ReLU(activation1) \n",
    "    if do_dropout:\n",
    "        output1 *= np.random.binomial([np.ones((len(x_i),hidden_dim))], \n",
    "        1 - dropout_percent)[0]*(1.0/(1 - dropout_percent))\n",
    "        \n",
    "    # Output layer \n",
    "    # multiply output of all hidden layers with weight of output layer\n",
    "    activation2 = output1.dot(W2) \n",
    "    \n",
    "    # pass product of weights and activation2 to activation function \n",
    "    output2 = sigmoid(activation2) \n",
    "    \n",
    "    if train:\n",
    "        # update parameters\n",
    "        delta_2 = (output2- y_i)*(output2*(1-output2))\n",
    "        delta_1 = np.dot(delta_2, W2.transpose())*(output1*(1-output1))\n",
    "        grad_1 = x_i.transpose().dot(delta_1)\n",
    "        grad_2 = output1.transpose().dot(delta_2)\n",
    "        history1 = momentum*history1 + alpha*(grad_1)\n",
    "        history2 = momentum*history2 + alpha*(grad_2)\n",
    "        W1 -= history1\n",
    "        W2 -= history2\n",
    "    return output2\n",
    "'''\n",
    "def back_propagation(x, y): \n",
    "    global W1, W2, alpha, momentum, history1, history2\n",
    "    # hiden layer \n",
    "    z1 = x.dot(W1)# input from layer 1 \n",
    "    a1 = sigmoid(z1)# output of layer 2 \n",
    "    \n",
    "    # Output layer \n",
    "    z2 = a1.dot(W2)# input of out layer \n",
    "    a2 = sigmoid(z2)# output of out layer \n",
    "    # error in output layer \n",
    "    d2 =(a2-y) \n",
    "    d1 = np.multiply((W2.dot((d2.transpose()))).transpose(), \n",
    "                        (np.multiply(a1, 1-a1))) \n",
    "\n",
    "    # Gradient for w1 and w2 \n",
    "    w1_adj = x.transpose().dot(d1) \n",
    "    w2_adj = a1.transpose().dot(d2) \n",
    "    \n",
    "    # Updating parameters\n",
    "    history1 = momentum*history1 + alpha*(w1_adj)\n",
    "    W1 = W1 - history1\n",
    "    history2 = momentum*history2 + alpha*(w2_adj)\n",
    "    W2 = W2 - history2\n",
    "\n",
    "    return(W1, W2) \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Design of a Neural Network from scratch\n",
    "\n",
    "*************<IMP>*************\n",
    "Mention hyperparameters used and describe functionality in detail in this space\n",
    "- carries 1 mark\n",
    "'''\n",
    "\n",
    "class NN:\n",
    "\n",
    "    ''' X and Y are dataframes '''\n",
    "    \n",
    "    def fit(self,X,Y):\n",
    "        '''\n",
    "        Function that trains the neural network by taking x_train and y_train samples as input\n",
    "        COMPLETE\n",
    "        '''\n",
    "        global epochs, alpha, initial_alpha, decay_rate, W1, W2, do_dropout, hidden_dim, patience, X_test, Y_test\n",
    "        x_train = []\n",
    "        for index, row in X.iterrows():\n",
    "            x_train.append(np.array(row, np.longdouble).reshape(1, len(row)))\n",
    "        y_train = np.array(Y)\n",
    "        x_test = []\n",
    "        for index, row in X_test.iterrows():\n",
    "            x_test.append(np.array(row, np.longdouble).reshape(1, len(row)))\n",
    "        y_test = np.array(Y_test)\n",
    "        acc_vals =[0]\n",
    "        loss_vals = [float('+inf')]\n",
    "        being_patient = 0\n",
    "        W1_copy = W1\n",
    "        W2_copy = W2\n",
    "        minloss = 0\n",
    "        test_loss_vals = [float('+inf')]\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss_vals =[] \n",
    "            epoch_test_loss_vals = []\n",
    "            y_hats = []\n",
    "            for i in range(len(x_train)):\n",
    "                # each x_train[i] is a row, input x[i]\n",
    "                y_pred = feed_forward_prop_backward(x_train[i], do_dropout, y_train[i], True)\n",
    "                y_hats.append(y_pred)\n",
    "            # decay the learning rate at each epoch\n",
    "            alpha = initial_alpha*(1/(1+decay_rate*(epoch+1)))\n",
    "            # calculate epoch loss\n",
    "            epoch_loss = compute_loss(np.array(y_hats),y_train)  \n",
    "            epoch_acc = compute_acc(y_hats, y_train)\n",
    "            print(\"Epochs:\", epoch + 1, \"==== acc:\", epoch_acc,\"=== loss:\",epoch_loss) \n",
    "            acc_vals.append(epoch_acc) \n",
    "            loss_vals.append(epoch_loss)\n",
    "            # check prediction on test dataset\n",
    "            y_test_vals = self.predict(X_test)\n",
    "\n",
    "            # calculate test dataset loss during the epoch\n",
    "            test_loss = compute_loss(np.array(y_test_vals),y_test)\n",
    "            test_loss_vals.append(test_loss)\n",
    "            \n",
    "        print(\"Finished training\")\n",
    "        print(\"Final Training Accuracy = \",acc_vals[-1],\"Final Training Loss = \", loss_vals[-1])\n",
    "    def predict(self,X):\n",
    "\n",
    "        \"\"\"\n",
    "        The predict function performs a simple feed forward of weights\n",
    "        and outputs yhat values \n",
    "\n",
    "        yhat is a list of the predicted value for df X\n",
    "        INCOMPLETE\n",
    "        \"\"\"\n",
    "        global Y_test\n",
    "        x_test = []\n",
    "        for index, row in X_test.iterrows():\n",
    "            x_test.append(np.array(row, np.longdouble).reshape(1, len(row)))\n",
    "        y_test = np.array(Y_test)\n",
    "        y_hat = []\n",
    "        for i in range(len(x_test)):\n",
    "            y_hat.append(feed_forward_prop_backward(x_test[i],False,y_test[i],False))\n",
    "        return y_hat\n",
    "        \n",
    "        '''\n",
    "        global W1, W2, Y_test\n",
    "        y_hat = []\n",
    "        #loss_vals = []\n",
    "        for index, row in X.iterrows():\n",
    "            y_pred = feed_forward(np.array(row, dtype=np.longdouble).reshape(1,len(row)),False)\n",
    "            #loss_vals.append(loss(y_pred,Y_test[index],2))\n",
    "            #y_pred = 0 if y_pred[0][0] > y_pred[0][1] else 1\n",
    "            y_hat.append(y_pred)\n",
    "        #loss_val = (sum(loss_vals)/len(X))*100\n",
    "        #acc_val = 100.0 - loss_val\n",
    "        #print(\"Testing accuracy:\",acc_val, \"Testing loss\",loss_val)\n",
    "        return y_hat\n",
    "        '''\n",
    "        \n",
    "        \n",
    "\n",
    "    def CM(self, y_test ,y_test_obs):\n",
    "        '''\n",
    "        Prints confusion matrix \n",
    "        y_test is list of y values in the test dataset\n",
    "        y_test_obs is list of y values predicted by the model\n",
    "\n",
    "        '''\n",
    "        \n",
    "        for i in range(len(y_test_obs)):\n",
    "            if(y_test_obs[i].flatten()[0]>0.6):\n",
    "                y_test_obs[i]=1\n",
    "            else:\n",
    "                y_test_obs[i]=0\n",
    "        print(\"Y_test_obs\",y_test_obs)\n",
    "        cm=[[0,0],[0,0]]\n",
    "        fp=0\n",
    "        fn=0\n",
    "        tp=0\n",
    "        tn=0\n",
    "        for i in range(len(y_test)):\n",
    "            if(y_test[i]==1 and y_test_obs[i]==1):\n",
    "                tp=tp+1\n",
    "            if(y_test[i]==0 and y_test_obs[i]==0):\n",
    "                tn=tn+1\n",
    "            if(y_test[i]==1 and y_test_obs[i]==0):\n",
    "                fp=fp+1\n",
    "            if(y_test[i]==0 and y_test_obs[i]==1):\n",
    "                fn=fn+1\n",
    "        cm[0][0]=tn\n",
    "        cm[0][1]=fp\n",
    "        cm[1][0]=fn\n",
    "        cm[1][1]=tp\n",
    "\n",
    "        p= tp/(max(1,tp+fp))\n",
    "        r=tp/max(1,tp+fn)\n",
    "        f1=(2*p*r)/max(1,p+r)\n",
    "        \n",
    "        print(\"Confusion Matrix : \")\n",
    "        print(cm)\n",
    "        print(\"\\n\")\n",
    "        print(f\"Precision : {p}\")\n",
    "        print(f\"Recall : {r}\")\n",
    "        print(f\"F1 SCORE : {f1}\")\n",
    "        print(\"Accuracy : \", (tp+tn)/(tp+fp+tn+fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shubham\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Shubham\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Shubham\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Shubham\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Shubham\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Shubham\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Community</th>\n",
       "      <th>Age</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Delivery phase</th>\n",
       "      <th>HB</th>\n",
       "      <th>IFA</th>\n",
       "      <th>BP</th>\n",
       "      <th>Education</th>\n",
       "      <th>Residence</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.00000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.0</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.172043</td>\n",
       "      <td>23.72043</td>\n",
       "      <td>45.328554</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>9.063441</td>\n",
       "      <td>0.698925</td>\n",
       "      <td>1.763257</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.139785</td>\n",
       "      <td>0.752688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.221400</td>\n",
       "      <td>3.25164</td>\n",
       "      <td>7.843697</td>\n",
       "      <td>0.145848</td>\n",
       "      <td>0.716845</td>\n",
       "      <td>0.461212</td>\n",
       "      <td>1.486694</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.348643</td>\n",
       "      <td>0.433788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.00000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.00000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>24.00000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.542233</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>26.00000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>38.00000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.875000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Community       Age     Weight  Delivery phase         HB        IFA  \\\n",
       "count  93.000000  93.00000  93.000000       93.000000  93.000000  93.000000   \n",
       "mean    2.172043  23.72043  45.328554        0.021505   9.063441   0.698925   \n",
       "std     1.221400   3.25164   7.843697        0.145848   0.716845   0.461212   \n",
       "min     1.000000  17.00000  30.000000        0.000000   5.900000   0.000000   \n",
       "25%     1.000000  21.00000  40.000000        0.000000   9.000000   0.000000   \n",
       "50%     2.000000  24.00000  44.000000        0.000000   9.000000   1.000000   \n",
       "75%     3.000000  26.00000  50.000000        0.000000   9.200000   1.000000   \n",
       "max     4.000000  38.00000  65.000000        1.000000  11.000000   1.000000   \n",
       "\n",
       "              BP  Education  Residence     Result  \n",
       "count  93.000000       93.0  93.000000  93.000000  \n",
       "mean    1.763257        5.0   1.139785   0.752688  \n",
       "std     1.486694        0.0   0.348643   0.433788  \n",
       "min     1.200000        5.0   1.000000   0.000000  \n",
       "25%     1.375000        5.0   1.000000   1.000000  \n",
       "50%     1.542233        5.0   1.000000   1.000000  \n",
       "75%     1.625000        5.0   1.000000   1.000000  \n",
       "max    13.875000        5.0   2.000000   1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = r\"F:/engi_books/sem5/mi/UE18CS303_Assignment/Assignment3/LBW_Dataset.csv\"\n",
    "raw_df=pd.read_csv(PATH)\n",
    "df = cleaning(raw_df)\n",
    "df \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    70\n",
       "0    23\n",
       "Name: Result, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Result\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df[\"Result\"]\n",
    "# drop y column\n",
    "#df.drop([\"Result\"],axis = 1, inplace = True)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df.drop([\"Result\"],axis = 1), Y,\n",
    "                                                    test_size = 0.3, random_state = 107)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "alpha = 0.07\n",
    "initial_alpha = 0.07\n",
    "n_train = X_test.shape[0]\n",
    "momentum = 0.99\n",
    "history1, history2 = 0,0\n",
    "decay_rate = 0.00001\n",
    "do_dropout = True\n",
    "hidden_dim = 10\n",
    "dropout_percent = 0.2\n",
    "patience = 50\n",
    "#print(X_train.shape[1])\n",
    "W1 = generate_wt(X_train.shape[1],hidden_dim)\n",
    "W2 = generate_wt(hidden_dim,1)\n",
    "#print(W1,\"\\n\\n\",W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 ==== acc: 0.27692307692307694 === loss: 1177.1732833335823\n",
      "Epochs: 2 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 3 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 4 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 5 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 6 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 7 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 8 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 9 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 10 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 11 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 12 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 13 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 14 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 15 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 16 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 17 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 18 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 19 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 20 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 21 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 22 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 23 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 24 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 25 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 26 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 27 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 28 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 29 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 30 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 31 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 32 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 33 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 34 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 35 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 36 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 37 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 38 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 39 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 40 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 41 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 42 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 43 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 44 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 45 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 46 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 47 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 48 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 49 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 50 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 51 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 52 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 53 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 54 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 55 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 56 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 57 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 58 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 59 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 60 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 61 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 62 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 63 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 64 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 65 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 66 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 67 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 68 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 69 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 70 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 71 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 72 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 73 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 74 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 75 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 76 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 77 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 78 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 79 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 80 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 81 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 82 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 83 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 84 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 85 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 86 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 87 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 88 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 89 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 90 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 91 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 92 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 93 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 94 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 95 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 96 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 97 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 98 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 99 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Epochs: 100 ==== acc: 0.26153846153846155 === loss: 528.125\n",
      "Finished training\n",
      "Final Training Accuracy =  0.26153846153846155 Final Training Loss =  528.125\n"
     ]
    }
   ],
   "source": [
    "neural_network = NN()\n",
    "neural_network.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy == 0.21428571428571427 Testing Loss == 98.0\n"
     ]
    }
   ],
   "source": [
    "y_pred_vals = neural_network.predict(X_test)\n",
    "epoch_loss = compute_loss(np.array(y_pred_vals),Y_test.to_numpy())  \n",
    "epoch_acc = compute_acc(y_pred_vals, Y_test.to_numpy())\n",
    "print(\"Testing Accuracy ==\",epoch_acc, \"Testing Loss ==\",epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_test_obs [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Confusion Matrix : \n",
      "[[6, 22], [0, 0]]\n",
      "\n",
      "\n",
      "Precision : 0.0\n",
      "Recall : 0.0\n",
      "F1 SCORE : 0.0\n",
      "Accuracy :  0.21428571428571427\n"
     ]
    }
   ],
   "source": [
    "y_probs = neural_network.predict(X_test)\n",
    "#print(len(y_probs), len(Y_test))\n",
    "y_hat = []\n",
    "#for i,vals in enumerate(y_probs):\n",
    "    #y_hat.append(0 if vals[0][0] > vals[0][1] else 1)\n",
    "#print(\"y_test \",y_test)\n",
    "neural_network.CM(Y_test.tolist(),y_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65, 9)\n",
      "(1, 9)\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "print(X_train.shape)\n",
    "for index, row in X_train.iterrows():\n",
    "    x.append(np.array(row).reshape(1, len(row)))\n",
    "print(x[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    with open(\"W1_weights.npy\",\"wb\") as f:\n",
    "        np.save(f,W1)\n",
    "    with open(\"W2_weights.npy\",\"wb\") as f:\n",
    "        np.save(f, W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06993006993006995\n"
     ]
    }
   ],
   "source": [
    "print(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
